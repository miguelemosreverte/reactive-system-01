# Version: 1.0.0
# Build args are injected at build time, env vars at runtime
x-version: &version "1.0.0"

# ============================================
# Tracing Strategy
# ============================================
# Default: Production-safe sampling (~0.1% of requests)
# On-demand: Add X-Debug:true header for full tracing on any request
#
# This means:
#   - Normal requests: ~0.1% are traced (low overhead, no OOM)
#   - Debug requests:  Always fully traced with detailed logs

services:
  # ============================================
  # Observability Infrastructure
  # ============================================

  otel-collector:
    build:
      context: ./platform/deployment/docker/observability/otel-collector
      dockerfile: Dockerfile
    container_name: reactive-otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./platform/deployment/docker/observability/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8889:8889"   # Prometheus metrics
      - "13133:13133" # Health check
    environment:
      - OTEL_FEATURE_ENABLED=${OTEL_FEATURE_ENABLED:-true}
      - GOMEMLIMIT=600MiB   # Increased: was hitting memory limit and refusing data
    deploy:
      resources:
        limits:
          memory: 768M  # 74% utilization hitting soft limits
          cpus: '3.0'  # Increased: was at 97% CPU during load
    depends_on:
      jaeger:
        condition: service_healthy
      loki:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - reactive-network

  jaeger:
    image: jaegertracing/all-in-one:1.53
    container_name: reactive-jaeger
    ports:
      - "16686:16686" # Jaeger UI
      - "14250:14250" # gRPC
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      # Memory storage - balanced for dev benchmarks
      - SPAN_STORAGE_TYPE=memory
      - MEMORY_MAX_TRACES=1000    # Reduced - only need sample traces
      - GOMAXPROCS=1
      - GOMEMLIMIT=50MiB          # 11MB actual usage at idle
    deploy:
      resources:
        limits:
          memory: 96M   # Right-sized: 46MB under load + headroom
          cpus: '1.5'
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:16686"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - reactive-network

  loki:
    image: grafana/loki:2.9.3
    container_name: reactive-loki
    ports:
      - "3100:3100"
    volumes:
      - ./platform/deployment/docker/observability/loki/loki-config.yaml:/etc/loki/loki-config.yaml:ro
      - loki-data:/loki
    command: -config.file=/etc/loki/loki-config.yaml
    deploy:
      resources:
        limits:
          memory: 256M  # Increased: was at 92% (177M/192M)
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - reactive-network

  promtail:
    image: grafana/promtail:2.9.3
    container_name: reactive-promtail
    volumes:
      - ./platform/deployment/docker/observability/promtail/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yaml
    deploy:
      resources:
        limits:
          memory: 96M  # Tight: 50MB actual
          cpus: '0.25'
    depends_on:
      loki:
        condition: service_healthy
    networks:
      - reactive-network

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: reactive-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./platform/deployment/docker/observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=1h'
      - '--storage.tsdb.retention.size=500MB'
    deploy:
      resources:
        limits:
          memory: 640M  # Increased: was at 77% (394M/512M), now 640M for headroom
          cpus: '1.0'
    depends_on:
      otel-collector:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - reactive-network

  grafana:
    image: grafana/grafana:10.2.3
    container_name: reactive-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      # Set the default home dashboard
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/provisioning/dashboards/json/reactive-system-overview.json
      # Disable login page for anonymous users
      - GF_AUTH_DISABLE_LOGIN_FORM=false
    deploy:
      resources:
        limits:
          memory: 192M  # Increased: was at 87% utilization
          cpus: '0.25'
    volumes:
      - ./platform/deployment/docker/observability/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
      - jaeger
      - loki
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - reactive-network

  # ============================================
  # Infrastructure Services
  # ============================================

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: reactive-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_HEAP_OPTS: "-Xmx192m -Xms192m"
    ports:
      - "2181:2181"
    deploy:
      resources:
        limits:
          memory: 288M  # 192MB heap + JVM overhead
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - reactive-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: reactive-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 2048M  # Increased: was at 72% (1.27G/1.79G), now 2GB for headroom
    networks:
      - reactive-network

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: reactive-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      echo 'Creating Kafka topics with development retention settings...'
      echo 'Retention: 5 minutes (300000ms), 100MB per partition'

      kafka-topics --create --if-not-exists --topic counter-events --bootstrap-server kafka:29092 --partitions 8 --replication-factor 1 --config retention.ms=300000 --config retention.bytes=104857600 --config segment.ms=60000

      kafka-topics --create --if-not-exists --topic counter-results --bootstrap-server kafka:29092 --partitions 8 --replication-factor 1 --config retention.ms=300000 --config retention.bytes=104857600 --config segment.ms=60000

      kafka-topics --create --if-not-exists --topic counter-alerts --bootstrap-server kafka:29092 --partitions 8 --replication-factor 1 --config retention.ms=300000 --config retention.bytes=104857600 --config segment.ms=60000

      echo 'Updating retention on existing topics...'
      kafka-configs --bootstrap-server kafka:29092 --alter --entity-type topics --entity-name counter-events --add-config retention.ms=300000,retention.bytes=104857600,segment.ms=60000 2>/dev/null || true
      kafka-configs --bootstrap-server kafka:29092 --alter --entity-type topics --entity-name counter-results --add-config retention.ms=300000,retention.bytes=104857600,segment.ms=60000 2>/dev/null || true
      kafka-configs --bootstrap-server kafka:29092 --alter --entity-type topics --entity-name counter-alerts --add-config retention.ms=300000,retention.bytes=104857600,segment.ms=60000 2>/dev/null || true

      echo 'Kafka topics configured with 5-minute retention (dev mode)!'
      "
    networks:
      - reactive-network

  # ============================================
  # Drools Service (Business Rules Engine)
  # ============================================

  drools:
    build:
      context: .
      dockerfile: platform/deployment/docker/drools/Dockerfile
      args:
        SERVICE_VERSION: *version
    container_name: reactive-drools
    ports:
      - "8180:8080"
    environment:
      - JAVA_OPTS=-Xmx768m -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -XX:ParallelGCThreads=2
      - SERVICE_VERSION=1.0.0
      # OpenTelemetry Configuration
      # Sampling controlled at application level via X-Debug header
      - OTEL_ENABLED=${OTEL_FEATURE_ENABLED:-true}
      - OTEL_SERVICE_NAME=drools
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_METRICS_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=none
      - OTEL_EXPORTER_OTLP_TIMEOUT=10000
      - OTEL_PROPAGATORS=tracecontext,baggage
    deploy:
      resources:
        limits:
          memory: 1024M  # 768MB heap + JVM overhead
          cpus: '2.5'
    depends_on:
      otel-collector:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - reactive-network

  # ============================================
  # Flink Service (Stream Processing)
  # ============================================

  flink-jobmanager:
    build:
      context: .
      dockerfile: platform/deployment/docker/flink/Dockerfile
    container_name: reactive-flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        parallelism.default: 8
      # OpenTelemetry Configuration
      # Sampling controlled at application level via X-Debug header
      - OTEL_ENABLED=${OTEL_FEATURE_ENABLED:-true}
      - OTEL_SERVICE_NAME=flink-jobmanager
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
    depends_on:
      kafka:
        condition: service_healthy
      drools:
        condition: service_healthy
      otel-collector:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1280M  # Increased: was at 72% (723M/1024M)
    networks:
      - reactive-network

  flink-taskmanager:
    build:
      context: .
      dockerfile: platform/deployment/docker/flink/Dockerfile
    # No container_name to allow scaling with: docker compose up -d --scale flink-taskmanager=2
    depends_on:
      flink-jobmanager:
        condition: service_started
      otel-collector:
        condition: service_healthy
    command: taskmanager
    volumes:
      - ./diagnostics:/tmp/diagnostics  # Persist heap dumps and GC logs
    # Note: Prometheus metrics port removed to allow scaling (use port range if needed)
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 8
        parallelism.default: 8
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9249
        taskmanager.network.memory.buffer-debloat.enabled: true
        taskmanager.network.memory.buffer-debloat.target: 50ms
        taskmanager.network.memory.buffer-debloat.period: 25ms
        execution.buffer-timeout: 1ms
        taskmanager.memory.process.size: 1280m
        env.java.opts.taskmanager: -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/diagnostics/heapdump.hprof -Xlog:gc*:file=/tmp/diagnostics/gc.log:time,uptime,level,tags -XX:NativeMemoryTracking=summary --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED
      # System-wide latency bounds (CQRS snapshot evaluation)
      - LATENCY_MIN_MS=${LATENCY_MIN_MS:-20}
      - LATENCY_MAX_MS=${LATENCY_MAX_MS:-100}
      - ALERTS_TOPIC=${COUNTER_ALERTS_TOPIC:-counter-alerts}
      - DROOLS_URL=http://drools:8080
      # Kafka consumer tuning for high throughput
      - KAFKA_FETCH_MAX_WAIT_MS=1
      - FLINK_BUFFER_TIMEOUT_MS=1
      # Async capacity for Drools calls (CQRS bounded snapshots)
      - ASYNC_CAPACITY=250
      # Benchmark mode: Skip Drools for Layer 2 benchmarks
      - SKIP_DROOLS=${SKIP_DROOLS:-false}
      # OpenTelemetry Configuration
      # Flink uses always_on to connect async traces across pipeline
      - OTEL_ENABLED=${OTEL_FEATURE_ENABLED:-true}
      - OTEL_SERVICE_NAME=flink-taskmanager
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_EXPORTER_OTLP_PROTOCOL=grpc
    deploy:
      resources:
        limits:
          memory: 1536M  # process.size=1280m + overhead
          cpus: '4.0'
    networks:
      - reactive-network

  flink-job-submitter:
    build:
      context: .
      dockerfile: platform/deployment/docker/flink/Dockerfile
    container_name: reactive-flink-job-submitter
    depends_on:
      flink-jobmanager:
        condition: service_healthy
      flink-taskmanager:
        condition: service_started
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      echo 'Waiting for Flink cluster to be fully ready...'

      # Wait for JobManager REST API to be available
      max_attempts=30
      attempt=0
      until curl -sf http://flink-jobmanager:8081/overview > /dev/null 2>&1; do
        attempt=$$((attempt + 1))
        if [ $$attempt -ge $$max_attempts ]; then
          echo 'ERROR: Flink JobManager not ready after max attempts'
          exit 1
        fi
        echo \"Waiting for Flink JobManager... (attempt $$attempt/$$max_attempts)\"
        sleep 2
      done

      # Wait for at least one TaskManager to register
      until curl -sf http://flink-jobmanager:8081/taskmanagers | grep -q 'taskmanagers'; do
        echo 'Waiting for TaskManager to register...'
        sleep 2
      done

      echo 'Flink cluster is ready!'
      echo 'Submitting Flink job...'
      /opt/flink/bin/flink run -m flink-jobmanager:8081 -d /opt/flink/jobs/counter-job.jar

      if [ $$? -eq 0 ]; then
        echo 'Flink job submitted successfully!'
      else
        echo 'ERROR: Failed to submit Flink job'
        exit 1
      fi

      tail -f /dev/null
      "
    environment:
      # Parallelism matches taskmanager slots (8 slots, parallelism 4 = 8 total tasks)
      - FLINK_PARALLELISM=8  # Increased: 2 taskmanagers Ã— 8 slots = 16 slots, parallelism 8 uses 16 (2 operator chains)
      - ASYNC_CAPACITY=250
      # Kafka consumer tuning for high throughput
      - KAFKA_FETCH_MAX_WAIT_MS=1
      - FLINK_BUFFER_TIMEOUT_MS=1
      # System-wide latency bounds
      - LATENCY_MIN_MS=${LATENCY_MIN_MS:-20}
      - LATENCY_MAX_MS=${LATENCY_MAX_MS:-100}
      - DROOLS_URL=http://drools:8080
      - ALERTS_TOPIC=${COUNTER_ALERTS_TOPIC:-counter-alerts}
      # Benchmark mode: Skip Drools for Layer 2 benchmarks
      - SKIP_DROOLS=${SKIP_DROOLS:-false}
    networks:
      - reactive-network

  # ============================================
  # Application Service (Counter Domain)
  # ============================================

  gateway:
    build:
      context: .
      dockerfile: application/Dockerfile
    container_name: reactive-application
    ports:
      - "8080:3000"
    environment:
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SERVICE_VERSION=1.0.0
      # OpenTelemetry Configuration
      # Sampling controlled at application level via X-Debug header
      - OTEL_ENABLED=${OTEL_FEATURE_ENABLED:-true}
      - OTEL_SERVICE_NAME=counter-application
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      # JVM tuning - G1GC optimized for throughput
      - JAVA_OPTS=-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -Xms1536m -Xmx1536m -XX:+AlwaysPreTouch -XX:ParallelGCThreads=4 -XX:ConcGCThreads=2 -XX:G1HeapRegionSize=4m
    deploy:
      resources:
        limits:
          memory: 2560M  # was at 92% (1.8G/2G), now 2.5GB for headroom
          cpus: '5.0'  # Increased: was at 240% CPU (60% of 4 cores)
    depends_on:
      kafka:
        condition: service_healthy
      otel-collector:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - reactive-network

  # ============================================
  # UI Service (React Frontend)
  # ============================================

  ui:
    build:
      context: ./platform/ui
      dockerfile: Dockerfile
    container_name: reactive-ui
    ports:
      - "3000:80"
    depends_on:
      - gateway
    deploy:
      resources:
        limits:
          memory: 32M  # Tight: 5MB actual (nginx serving static files)
          cpus: '0.25'
    volumes:
      - ./DOCUMENTATION.md:/usr/share/nginx/docs/DOCUMENTATION.md:ro
      - ./platform/ui/DOCUMENTATION.md:/usr/share/nginx/docs/ui/DOCUMENTATION.md:ro
      - ./platform/deployment/docker/gateway/DOCUMENTATION.md:/usr/share/nginx/docs/gateway/DOCUMENTATION.md:ro
      - ./platform/deployment/docker/flink/DOCUMENTATION.md:/usr/share/nginx/docs/flink/DOCUMENTATION.md:ro
      - ./platform/deployment/docker/drools/DOCUMENTATION.md:/usr/share/nginx/docs/drools/DOCUMENTATION.md:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - reactive-network

# ============================================
  # cAdvisor (Container Metrics)
  # ============================================

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: reactive-cadvisor
    privileged: true
    ports:
      - "8085:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    deploy:
      resources:
        limits:
          memory: 96M  # Tight: 29MB actual
          cpus: '0.25'
    networks:
      - reactive-network

networks:
  reactive-network:
    driver: bridge

volumes:
  grafana-data:
  prometheus-data:
  loki-data:
  jaeger-badger:
